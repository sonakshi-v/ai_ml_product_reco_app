{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Furniture Product Recommendation Model Training\n",
    "\n",
    "This notebook implements the ML models for product recommendation, NLP clustering, CV classification, and GenAI description generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pinecone\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "df = pd.read_csv('../data/furniture_dataset_cleaned.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Embedding and Recommendation Model\n",
    "\n",
    "Using SentenceTransformers to create embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create combined text for embedding\n",
    "df['text_for_embedding'] = df['title'] + '. ' + df['description']\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating text embeddings...\")\n",
    "embeddings = model.encode(df['text_for_embedding'].tolist(), show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('../models/text_embeddings.npy', embeddings)\n",
    "print(\"Embeddings saved to ../models/text_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP Clustering for Similar Products\n",
    "\n",
    "Using K-means clustering on text embeddings to group similar products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters using silhouette score\n",
    "silhouette_scores = []\n",
    "k_range = range(5, 21, 2)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"k={k}, Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal k (highest silhouette score)\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Fit final model\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Save cluster model\n",
    "import joblib\n",
    "joblib.dump(kmeans, '../models/kmeans_model.pkl')\n",
    "print(\"K-means model saved to ../models/kmeans_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "cluster_counts = df['cluster'].value_counts().sort_index()\n",
    "print(\"Cluster distribution:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# Sample products from each cluster\n",
    "for cluster_id in range(min(5, optimal_k)):\n",
    "    print(f\"\\nCluster {cluster_id} sample products:\")\n",
    "    cluster_products = df[df['cluster'] == cluster_id]['title'].head(3)\n",
    "    for product in cluster_products:\n",
    "        print(f\"- {product[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computer Vision Model for Image Classification\n",
    "\n",
    "Using a pre-trained Vision Transformer for category classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image classification pipeline\n",
    "image_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Function to classify image\n",
    "def classify_image(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        # Resize if too large\n",
    "        if img.size[0] > 224 or img.size[1] > 224:\n",
    "            img.thumbnail((224, 224))\n",
    "        results = image_classifier(img)\n",
    "        return results[0]['label']\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying image {image_url}: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Classify images for products with valid URLs\n",
    "print(\"Classifying product images...\")\n",
    "df['image_category'] = df['images'].apply(lambda x: classify_image(x) if pd.notnull(x) else \"No Image\")\n",
    "\n",
    "# Save updated dataframe\n",
    "df.to_csv('../data/furniture_dataset_with_categories.csv', index=False)\n",
    "print(\"Dataset with image categories saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image categories\n",
    "image_cat_counts = df['image_category'].value_counts().head(10)\n",
    "print(\"Top image categories:\")\n",
    "print(image_cat_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=image_cat_counts.values, y=image_cat_counts.index)\n",
    "plt.title('Top 10 Image Categories')\n",
    "plt.xlabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generative AI for Creative Descriptions\n",
    "\n",
    "Using LangChain with a HuggingFace model for generating creative product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\", max_length=100, temperature=0.7)\n",
    "\n",
    "# Create LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=text_generator)\n",
    "\n",
    "# Create prompt template for creative descriptions\n",
    "description_prompt = PromptTemplate(\n",
    "    input_variables=[\"product_title\", \"original_description\"],\n",
    "    template=\"\"\"Create a creative and engaging product description for:\n",
    "Product: {product_title}\n",
    "Original: {original_description}\n",
    "\n",
    "Creative Description:\"\"\"\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "description_chain = LLMChain(llm=llm, prompt=description_prompt)\n",
    "\n",
    "# Function to generate creative description\n",
    "def generate_creative_description(title, original_desc):\n",
    "    try:\n",
    "        result = description_chain.run(product_title=title, original_description=original_desc)\n",
    "        # Clean up the generated text\n",
    "        creative_desc = result.split('Creative Description:')[-1].strip()\n",
    "        return creative_desc\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description: {e}\")\n",
    "        return original_desc\n",
    "\n",
    "# Generate creative descriptions for a sample\n",
    "print(\"Generating creative descriptions for sample products...\")\n",
    "sample_df = df.head(10).copy()\n",
    "sample_df['creative_description'] = sample_df.apply(\n",
    "    lambda row: generate_creative_description(row['title'], row['description']), axis=1\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"\\nOriginal: {row['description'][:100]}...\")\n",
    "    print(f\"Creative: {row['creative_description'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pinecone Vector Database Setup\n",
    "\n",
    "Store embeddings in Pinecone for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone (you'll need to set your API key)\n",
    "# pinecone.init(api_key=\"your-api-key\", environment=\"your-environment\")\n",
    "\n",
    "# Create index\n",
    "# index_name = \"furniture-recommendations\"\n",
    "# if index_name not in pinecone.list_indexes():\n",
    "#     pinecone.create_index(index_name, dimension=embeddings.shape[1])\n",
    "\n",
    "# index = pinecone.Index(index_name)\n",
    "\n",
    "# Prepare data for Pinecone\n",
    "# vectors = []\n",
    "# for i, (idx, row) in enumerate(df.iterrows()):\n",
    "#     vector = {\n",
    "#         \"id\": row['uniq_id'],\n",
    "#         \"values\": embeddings[i].tolist(),\n",
    "#         \"metadata\": {\n",
    "#             \"title\": row['title'],\n",
    "#             \"description\": row['description'],\n",
    "#             \"price\": float(row['price']),\n",
    "#             \"brand\": row['brand'],\n",
    "#             \"categories\": row['categories'],\n",
    "#             \"image\": row['images'],\n",
    "#             \"cluster\": int(row['cluster'])\n",
    "#         }\n",
    "#     }\n",
    "#     vectors.append(vector)\n",
    "\n",
    "# Upload to Pinecone in batches\n",
    "# batch_size = 100\n",
    "# for i in range(0, len(vectors), batch_size):\n",
    "#     batch = vectors[i:i+batch_size]\n",
    "#     index.upsert(vectors=batch)\n",
    "#     print(f\"Uploaded batch {i//batch_size + 1}/{len(vectors)//batch_size + 1}\")\n",
    "\n",
    "print(\"Pinecone setup code prepared. Uncomment and run with your API key.\")\n",
    "print(\"Note: Pinecone integration will be completed in the backend implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Training Summary:\")\n",
    "print(f\"- Dataset size: {len(df)} products\")\n",
    "print(f\"- Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"- Number of clusters: {optimal_k}\")\n",
    "print(f\"- Silhouette score: {max(silhouette_scores):.4f}\")\n",
    "print(f\"- Products with images: {df['images'].notnull().sum()}\")\n",
    "print(f\"- Image categories identified: {df['image_category'].nunique()}\")\n",
    "\n",
    "# Save final dataset\n",
    "df.to_csv('../data/furniture_dataset_final.csv', index=False)\n",
    "print(\"\\nFinal dataset saved to ../data/furniture_dataset_final.csv\")\n",
    "\n",
    "# Save model artifacts\n",
    "model.save('../models/sentence_transformer_model')\n",
    "print(\"Sentence transformer model saved to ../models/sentence_transformer_model\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Set up Pinecone API key and run vector database setup\")\n",
    "print(\"2. Implement backend API endpoints\")\n",
    "print(\"3. Build frontend interface\")\n",
    "print(\"4. Test end-to-end functionality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
